{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "sys.path.append(\"../../../src/\")\n",
    "import data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    \"\"\"Set the seed for reproducibility across multiple libraries.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "william_dir = dict(\n",
    "    hotel = \"/raid/m13519061/ta/facebook-absa/data/absa/en/zhang/interim/interim_2/rest1516\"\n",
    ")\n",
    "\n",
    "william = dict(\n",
    "    hotel = dict(\n",
    "        train = data_utils.read_data(path=william_dir[\"hotel\"] + \"/train.txt\",\n",
    "                                     target_format=\"acos\"),\n",
    "        val = data_utils.read_data(path=william_dir[\"hotel\"] + \"/dev.txt\",\n",
    "                                     target_format=\"acos\"),\n",
    "        test = data_utils.read_data(path=william_dir[\"hotel\"] + \"/test.txt\",\n",
    "                                     target_format=\"acos\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. AOS (ASTE)\n",
    "    * AO\n",
    "    * AS\n",
    "    * A\n",
    "    * O\n",
    "\n",
    "2. ACS (TASD)\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * C\n",
    "\n",
    "3. ACOS\n",
    "    * AO\n",
    "    * AS\n",
    "    * CS\n",
    "    * A\n",
    "    * O\n",
    "    * C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_tree = {\n",
    "#     \"aos\" : [\"aos\",\"ao\",\"as\",'a','o'],\n",
    "#     \"asc\" : [\"asc\",\"as\",\"sc\",'a','c'],\n",
    "#     \"oasc\" : [\"oasc\",\"ao\",\"as\",\"sc\",'a','o','c']\n",
    "# }\n",
    "\n",
    "# all_task = []\n",
    "# for k,v1 in task_tree.items():\n",
    "#     if k not in all_task:\n",
    "#         all_task.append(k)\n",
    "#     for v2 in v1:\n",
    "#         if v2 not in all_task:\n",
    "#             all_task.append(v2)\n",
    "\n",
    "# print(all_task)\n",
    "\n",
    "tasks = {\n",
    "    \"single\" : ['a', 'o', 'c'],\n",
    "    \"simple\" : [\"ao\", \"as\", \"cs\"],\n",
    "    \"complex\" : [\"acos\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_tasks = [\n",
    "    tasks[\"simple\"],\n",
    "    tasks[\"complex\"],\n",
    "    tasks[\"single\"] + tasks[\"simple\"],\n",
    "    tasks[\"single\"] + tasks[\"complex\"],\n",
    "    tasks[\"simple\"] + tasks[\"complex\"],\n",
    "    tasks[\"single\"] + tasks[\"simple\"] + tasks[\"complex\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'o', 'ao', 'as', 'aos']\n"
     ]
    }
   ],
   "source": [
    "all_task = combination_tasks[-1]\n",
    "print(all_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sentiment': 'positive'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_utils.reduce_targets([{\n",
    "    \"aspect\" : \"NULL\",\n",
    "    \"opinion\" : \"kocak\",\n",
    "    \"sentiment\" : \"positive\"\n",
    "}],\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "# William (AOS ID)\n",
    "william_intermediate = dict()\n",
    "\n",
    "for domain, v1 in william.items():\n",
    "    william_intermediate[domain] = dict()\n",
    "    for task in all_task:\n",
    "        william_intermediate[domain][task] = dict()\n",
    "        for split in v1.keys():\n",
    "            ds = william[domain][split]\n",
    "            ds_copy = deepcopy(ds)\n",
    "            for i in range(len(ds_copy)):\n",
    "                # Reduce\n",
    "                ds_copy[i][\"target\"] = data_utils.reduce_targets(ds_copy[i][\"target\"],task)\n",
    "                # Remove Duplicates\n",
    "                ds_copy[i][\"target\"] = data_utils.remove_duplicate_targets(ds_copy[i][\"target\"])\n",
    "            william_intermediate[domain][task][split] = ds_copy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = \"<extra_id_X>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_tokens = {\n",
    "    ',' : \"<comma>\",\n",
    "    '(' : \"<open_bracket>\",\n",
    "    ')' : \"<close_bracket>\",\n",
    "    ';' : \"<semicolon>\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_answer(targets,se_order):\n",
    "    if len(targets) == 0:\n",
    "        return \"NULL\"\n",
    "    result = []\n",
    "    counter = 0\n",
    "    for t in targets:\n",
    "        constructed_t = \"\"\n",
    "        for se in se_order:\n",
    "            counter = counter % 100\n",
    "            constructed_t += ' ' + mask.replace('X',str(counter)) + ' ' + t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "            counter += 1\n",
    "        constructed_t = constructed_t.strip()\n",
    "        result.append(constructed_t)\n",
    "    result = \" ; \".join(result)\n",
    "    return result\n",
    "# def construct_answer(targets,se_order):\n",
    "#     if len(targets) == 0:\n",
    "#         return \"NULL\"\n",
    "#     result = []\n",
    "#     for t in targets:\n",
    "#         constructed_t = []\n",
    "#         for se in se_order:\n",
    "#             element = t[data_utils.SENTIMENT_ELEMENT[se]]\n",
    "#             for k, v in added_tokens.items():\n",
    "#                 element = element.replace(k,v)\n",
    "#             constructed_t.append(element)\n",
    "#         constructed_t = \" , \".join(constructed_t)\n",
    "#         constructed_t = f\"( {constructed_t} )\"\n",
    "#         result.append(constructed_t)\n",
    "#     result = \" ; \".join(result)\n",
    "#     return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(text,se_order):\n",
    "    pattern = []\n",
    "    for counter, se in enumerate(se_order):\n",
    "        pattern.append(data_utils.SENTIMENT_ELEMENT[se] + \" : \" + mask.replace('X',str(counter)))\n",
    "    pattern = \" ,\".join(pattern)\n",
    "    prompt = f\"Extract ABSA with format >> {pattern} | \"\n",
    "    # result = text + \"| \" + pattern\n",
    "    result = prompt + text\n",
    "    return result\n",
    "# def construct_prompt(text,se_order):\n",
    "#     prompt = []\n",
    "#     for se in se_order:\n",
    "#         prompt.append(data_utils.SENTIMENT_ELEMENT[se])\n",
    "#     prompt = \" , \".join(prompt)\n",
    "#     prompt = f\"( {prompt} )\"\n",
    "#     masked_text = text\n",
    "#     for k, v in added_tokens.items():\n",
    "#         masked_text = masked_text.replace(k,v)\n",
    "#     result = masked_text + \" | \" + prompt\n",
    "#     return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def catch_answer(output,se_order):\n",
    "    if output == \"NULL\":\n",
    "        return []\n",
    "    output = output.replace(\"<pad>\",'')\n",
    "    output = output.replace(\"</s>\",'')\n",
    "    pattern = r\"\"\n",
    "    for se in se_order:\n",
    "        if se != 's':\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\"\n",
    "        else:\n",
    "            pattern += f\"<extra_id_\\d+>\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\"\n",
    "    found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "    for i in range(len(found)):\n",
    "        for k, v in found[i].items():\n",
    "            found[i][k] = found[i][k].strip()\n",
    "    return found\n",
    "# def catch_answer(output,se_order):\n",
    "#     if output == \"NULL\":\n",
    "#         return []\n",
    "#     output = output.replace(\"<pad>\",'')\n",
    "#     output = output.replace(\"</s>\",'')\n",
    "#     pattern = []\n",
    "#     for se in se_order:\n",
    "#         if se != 's':\n",
    "#             pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT[se]}>[^;]+)\\s*\")\n",
    "#         else:\n",
    "#             pattern.append(f\"\\s*(?P<{data_utils.SENTIMENT_ELEMENT['s']}>positive|negative|neutral)\\s*\")\n",
    "#     pattern = ','.join(pattern)\n",
    "#     pattern = f\"\\({pattern}\\)\"\n",
    "#     found = [found_iter.groupdict() for found_iter in re.finditer(pattern,output)]\n",
    "#     for i in range(len(found)):\n",
    "#         for k, v in found[i].items():\n",
    "#             found[i][k] = found[i][k].strip()\n",
    "#     return found"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Tokenized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_args = {\n",
    "    \"max_length\" : 128,\n",
    "    \"padding\" : True,\n",
    "    \"truncation\" : True,\n",
    "    \"return_tensors\" : \"pt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer_id = AutoTokenizer.from_pretrained(\"google/mt5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_id(dataset):\n",
    "    result = tokenizer_id(dataset[\"input\"], text_target=dataset[\"output\"], **encoding_args)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "supporting_path = {\n",
    "    \"doc_sa\" : \"/raid/m13519061/ta/facebook-absa/data/doc_sa/en/kaggle/interim/Restaurant_Reviews.csv\",\n",
    "    \"pos_tag\" : \"../../../data/pos_tag/id/interim/data.csv\",\n",
    "    \"ner\" : \"/raid/m13519061/ta/facebook-absa/data/ner/en/jia/interim/interim_2/tech.csv\",\n",
    "    \"emotion\" : \"/raid/m13519061/ta/facebook-absa/data/emotion_cls/en/kaggle/interim/emotion.csv\"\n",
    "}\n",
    "\n",
    "supporting_df = {\n",
    "    k: pd.read_csv(v) for k,v in supporting_path.items()\n",
    "}\n",
    "\n",
    "n_sample_supporting_ds = np.inf\n",
    "for k, v in supporting_df.items():\n",
    "    if v.shape[0] < n_sample_supporting_ds:\n",
    "        n_sample_supporting_ds = v.shape[0]\n",
    "\n",
    "for k,v in supporting_df.items():\n",
    "    supporting_df[k] = v.sample(n_sample_supporting_ds,random_state=42).reset_index(drop=True)\n",
    "    supporting_df[k][\"task\"] = \"non_absa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'output', 'task'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dataset.from_pandas(supporting_df[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting_data_combination = []\n",
    "# for i in range(len(supporting_df.keys())):\n",
    "#     supporting_data_combination += list(combinations(supporting_df.keys(),i+1))\n",
    "# print(supporting_data_combination)\n",
    "supporting_data_combination = [\n",
    "                                ('doc_sa',), \n",
    "                               ('pos_tag',), \n",
    "                               ('ner',), \n",
    "                               ('emotion',),\n",
    "                               ('doc_sa', 'pos_tag'), \n",
    "                               ('doc_sa', 'ner'), \n",
    "                               ('doc_sa', 'emotion'),\n",
    "                               ('pos_tag', 'ner'), \n",
    "                               ('pos_tag', 'emotion'), \n",
    "                               ('ner', 'emotion'),\n",
    "                               ('doc_sa', 'pos_tag', 'ner'), \n",
    "                               ('doc_sa', 'pos_tag', 'emotion'),\n",
    "                               ('doc_sa', 'ner', 'emotion'), \n",
    "                               ('pos_tag', 'ner', 'emotion'),\n",
    "                               ('doc_sa', 'pos_tag', 'ner', 'emotion')\n",
    "                               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(supporting_data_combination))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1860</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Kurs &lt;extra_id_1&gt; noun ; &lt;extra_i...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Sementara &lt;extra_id_1&gt; coordinati...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Keberhasilan &lt;extra_id_1&gt; noun ; ...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; BI &lt;extra_id_1&gt; proper noun ; &lt;ex...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Menurut &lt;extra_id_1&gt; preposition ...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Kami &lt;extra_id_1&gt; personal pronou...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Padahal &lt;extra_id_1&gt; coordinating...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Kurs &lt;extra_id_1&gt; noun ; &lt;extra_i...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Tujuan &lt;extra_id_1&gt; noun ; &lt;extra...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>Ekstrak POS TAG dengan format &gt;&gt; pos : &lt;extra_...</td>\n",
       "      <td>&lt;extra_id_0&gt; Euro &lt;extra_id_1&gt; noun ; &lt;extra_i...</td>\n",
       "      <td>non_absa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  input  \\\n",
       "1860  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "353   Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "1333  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "905   Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "1289  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "...                                                 ...   \n",
       "1130  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "1294  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "860   Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "1459  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "1126  Ekstrak POS TAG dengan format >> pos : <extra_...   \n",
       "\n",
       "                                                 output      task  \n",
       "1860  <extra_id_0> Kurs <extra_id_1> noun ; <extra_i...  non_absa  \n",
       "353   <extra_id_0> Sementara <extra_id_1> coordinati...  non_absa  \n",
       "1333  <extra_id_0> Keberhasilan <extra_id_1> noun ; ...  non_absa  \n",
       "905   <extra_id_0> BI <extra_id_1> proper noun ; <ex...  non_absa  \n",
       "1289  <extra_id_0> Menurut <extra_id_1> preposition ...  non_absa  \n",
       "...                                                 ...       ...  \n",
       "1130  <extra_id_0> Kami <extra_id_1> personal pronou...  non_absa  \n",
       "1294  <extra_id_0> Padahal <extra_id_1> coordinating...  non_absa  \n",
       "860   <extra_id_0> Kurs <extra_id_1> noun ; <extra_i...  non_absa  \n",
       "1459  <extra_id_0> Tujuan <extra_id_1> noun ; <extra...  non_absa  \n",
       "1126  <extra_id_0> Euro <extra_id_1> noun ; <extra_i...  non_absa  \n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([supporting_df[\"ner\"],supporting_df[\"pos_tag\"]]).reset_index(drop=True).sample(frac=1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_2(tasks,combo_supporting_ds=[]):\n",
    "    william_2 = dict()\n",
    "    for domain, v1 in william_intermediate.items():\n",
    "        william_2[domain] = {\n",
    "            \"train\" : [], # basic task\n",
    "            \"val\" : [], # complex task\n",
    "            \"test\" : [] # complex task\n",
    "        }\n",
    "        # TRAIN\n",
    "        for basic_task in tasks:\n",
    "            for el in william_intermediate[domain][basic_task][\"train\"]:\n",
    "                william_2[domain][\"train\"].append({\n",
    "                        \"input\" : construct_prompt(el[\"text\"],basic_task),\n",
    "                        \"output\" : construct_answer(el[\"target\"],basic_task),\n",
    "                        \"task\" : basic_task\n",
    "                    })\n",
    "        # VAL\n",
    "        for el in william_intermediate[domain][\"aos\"][\"val\"]:\n",
    "            william_2[domain][\"val\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],\"aos\"),\n",
    "                    \"output\" : construct_answer(el[\"target\"],\"aos\"),\n",
    "                    \"task\" : \"aos\"\n",
    "                })\n",
    "        # TEST\n",
    "        for el in william_intermediate[domain][\"aos\"][\"test\"]:\n",
    "            william_2[domain][\"test\"].append({\n",
    "                    \"input\" : construct_prompt(el[\"text\"],\"aos\"),\n",
    "                    \"output\" : construct_answer(el[\"target\"],\"aos\"),\n",
    "                    \"task\" : \"aos\"\n",
    "                })\n",
    "        random.shuffle(william_2[domain][\"train\"])\n",
    "        random.shuffle(william_2[domain][\"val\"])\n",
    "        random.shuffle(william_2[domain][\"test\"])\n",
    "        william_2[domain][\"train\"] = pd.DataFrame(william_2[domain][\"train\"])\n",
    "        william_2[domain][\"val\"] = Dataset.from_list(william_2[domain][\"val\"])\n",
    "        william_2[domain][\"test\"] = Dataset.from_list(william_2[domain][\"test\"])\n",
    "\n",
    "        for ds_name in combo_supporting_ds:\n",
    "            # supporting_ds = Dataset.from_pandas(supporting_df[ds_name])\n",
    "            william_2[domain][\"train\"] = pd.concat([william_2[domain][\"train\"],supporting_df[ds_name]]).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "        william_2[domain][\"train\"] = Dataset.from_pandas(william_2[domain][\"train\"])\n",
    "    \n",
    "    william_tok = dict()\n",
    "    for domain, v1 in william_2.items():\n",
    "        william_tok[domain] = dict()\n",
    "        for split, v2 in v1.items():\n",
    "            if split != \"test\":\n",
    "                william_tok[domain][split] = william_2[domain][split].map(encode_id,batched=True,remove_columns=[\"input\",\"output\",\"task\"])\n",
    "                william_tok[domain][split].set_format(\"torch\")\n",
    "            else:\n",
    "                william_tok[domain][split] = encode_id(william_2[domain][split])\n",
    "    \n",
    "    return william_2, william_tok"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator_id = DataCollatorForSeq2Seq(tokenizer=tokenizer_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EvalPrediction\n",
    "from evaluation import recall, precision, f1_score, summary_score\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "\n",
    "def seperate_target_prediction_per_task(predictions:List[List[Dict]],targets:List[List[Dict]],tasks:List) -> Tuple[Dict[str,List],Dict[str,List]]:\n",
    "    per_task_targets = {}\n",
    "    per_task_predictions = {}\n",
    "    for target, prediction, task in zip(targets,predictions,tasks):\n",
    "        if task not in per_task_targets.keys():\n",
    "            per_task_targets[task] = []\n",
    "        if task not in per_task_predictions.keys():\n",
    "            per_task_predictions[task] = []\n",
    "        per_task_targets[task].append(target)\n",
    "        per_task_predictions[task].append(prediction)\n",
    "    return per_task_targets, per_task_predictions\n",
    "\n",
    "def preprocess_eval_preds(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer):\n",
    "    input_ids = eval_preds.inputs\n",
    "    target_ids = eval_preds.label_ids\n",
    "    pred_ids = eval_preds.predictions\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(input_ids, tuple):\n",
    "        input_ids = input_ids[0]\n",
    "    if isinstance(target_ids, tuple):\n",
    "        target_ids = target_ids[0]\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "    \n",
    "    input_ids = np.argmax(input_ids,axis=-1) if len(input_ids.shape) == 3 else input_ids # in case not predict with generate\n",
    "    target_ids = np.argmax(target_ids,axis=-1) if len(target_ids.shape) == 3 else target_ids # in case not predict with generate\n",
    "    prediction_ids = np.argmax(pred_ids,axis=-1) if len(pred_ids.shape) == 3 else pred_ids # in case not predict with generate\n",
    "\n",
    "    input_ids = [[token for token in row if token != -100] for row in input_ids]\n",
    "    target_ids = [[token for token in row if token != -100] for row in target_ids]\n",
    "    prediction_ids = [[token for token in row if token != -100] for row in prediction_ids]\n",
    "\n",
    "    inputs = tokenizer.batch_decode(input_ids,**decoding_args)\n",
    "    targets = tokenizer.batch_decode(target_ids,**decoding_args)\n",
    "    predictions = tokenizer.batch_decode(prediction_ids,**decoding_args)\n",
    "\n",
    "    return inputs, targets, predictions\n",
    "\n",
    "def compute_metrics(eval_preds:EvalPrediction,decoding_args:Dict[str,str],tokenizer:AutoTokenizer,tasks:List) -> Dict[str,float]: # MAY NOT BE SUFFICIATE FOR CAUSAL LM\n",
    "        \"\"\"\n",
    "        ### DESC\n",
    "            Method to compute the metrics.\n",
    "        ### PARAMS\n",
    "        * eval_preds: EvalPrediction instance from training.\n",
    "        * decoding_args: Decoding arguments.\n",
    "        ### RETURN\n",
    "        * metrics: Dictionary of metrics.\n",
    "        \"\"\"\n",
    "        inputs, targets, predictions = preprocess_eval_preds(eval_preds,decoding_args,tokenizer)\n",
    "\n",
    "        print(\"INPUTS >>\",inputs[0])\n",
    "        print(\"TARGETS >>\",targets[0])\n",
    "        print(\"PREDS >>\",predictions[0])\n",
    "\n",
    "        targets = [catch_answer(text,task) for text,task in zip(targets,tasks) if task != \"non_absa\"]\n",
    "        predictions = [catch_answer(text,task) for text,task in zip(predictions,tasks) if task != \"non_absa\"]\n",
    "\n",
    "        per_task_targets, per_task_predictions = seperate_target_prediction_per_task(predictions, targets, tasks)\n",
    "        \n",
    "        metrics = {}\n",
    "\n",
    "        metrics[\"overall_recall\"] = recall(predictions,targets)\n",
    "        metrics[\"overall_precision\"] = precision(predictions,targets)\n",
    "        metrics[\"overall_f1_score\"] = f1_score(predictions,targets)\n",
    "\n",
    "        for task in per_task_targets.keys():\n",
    "            if task == \"non_absa\":\n",
    "                continue\n",
    "            metrics[f\"{task}_recall\"] = recall(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_precision\"] = precision(per_task_predictions[task],per_task_targets[task])\n",
    "            metrics[f\"{task}_f1_score\"] = f1_score(per_task_predictions[task],per_task_targets[task])\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,\n",
    "    \"save_total_limit\": 2,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"logging_strategy\" : \"epoch\",\n",
    "    \"metric_for_best_model\": \"overall_f1_score\",\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"adam_epsilon\": 1e-08,\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"logging_dir\" : \"./output/log\",\n",
    "    \"include_inputs_for_metrics\" : True\n",
    "}\n",
    "\n",
    "train_args = Seq2SeqTrainingArguments(**train_args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "# trainer = {\n",
    "#     \"peng\" : {},\n",
    "#     \"wan\" : {},\n",
    "#     \"zhang\" : {},\n",
    "#     \"william\" : {}\n",
    "# }\n",
    "\n",
    "decoding_args = {\n",
    "    \"skip_special_tokens\" : False\n",
    "}\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, targets):\n",
    "    pred_logits = logits[0] if isinstance(logits,tuple) else logits\n",
    "    pred_ids = torch.argmax(pred_logits, dim=-1)\n",
    "    return pred_ids, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def generate_predictions(model,tokenizer,tokenized:torch.Tensor,device:torch.device=torch.device(\"cpu\"),batch_size:int=16,max_len:int=128,decoding_args:Dict={}) -> List[str]:\n",
    "    # Data loader\n",
    "    input_ids_data_loader = torch.utils.data.DataLoader(tokenized[\"input_ids\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    attention_mask_data_loader = torch.utils.data.DataLoader(tokenized[\"attention_mask\"],\n",
    "                        batch_size=batch_size,shuffle=False)\n",
    "    # Predict\n",
    "    model = model\n",
    "    tokenizer = tokenizer\n",
    "    tensor_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask in tqdm(zip(input_ids_data_loader,attention_mask_data_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            tensor_predictions.extend(model.generate(input_ids=input_ids,attention_mask=attention_mask,max_length=max_len,pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id).cpu())\n",
    "            input_ids = input_ids.cpu()\n",
    "            attention_mask = attention_mask.cpu()\n",
    "    tensor_predictions = [[token for token in row if token != -100] for row in tensor_predictions]\n",
    "    predictions = tokenizer.batch_decode(tensor_predictions,**decoding_args)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_result(str_preds_,preds,targets,filename):\n",
    "    result = []\n",
    "    str_preds = [el.replace(\"<pad>\",'').replace(\"</s>\",'') for el in str_preds_]\n",
    "    assert len(str_preds) == len(preds) == len(targets)\n",
    "    for i in range(len(str_preds)):\n",
    "        result.append({\n",
    "            \"str_pred\" : str_preds[i],\n",
    "            \"pred\" : preds[i],\n",
    "            \"target\" : targets[i]\n",
    "        })\n",
    "    \n",
    "    with open(filename,'w') as fp:\n",
    "        json.dump(result,fp)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# William Hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('doc_sa',)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supporting_data_combination[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16//n_gpu, 32//n_gpu, 64//n_gpu, 128//n_gpu]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.01)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial=None):\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-base\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# for combo_supporting_ds_name in supporting_data_combination:\n",
    "william_2, william_tok = create_data_2(all_task,[\"ner\",\"pos_tag\"])\n",
    "# for combo_task in combination_tasks:\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/mt5-base\")\n",
    "# model.to(device)\n",
    "model = model_init()\n",
    "trainer = Seq2SeqTrainer(\n",
    "        model = model,\n",
    "        # model_init=model_init,\n",
    "        args = train_args,\n",
    "        tokenizer = tokenizer_id,\n",
    "        data_collator = data_collator_id,\n",
    "        train_dataset = william_tok[\"hotel\"][\"train\"],\n",
    "        eval_dataset = william_tok[\"hotel\"][\"val\"],\n",
    "        compute_metrics = lambda eval_preds: compute_metrics(eval_preds,decoding_args,tokenizer_id,william_2[\"hotel\"][\"val\"][\"task\"]),\n",
    "        preprocess_logits_for_metrics = preprocess_logits_for_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_objective(metrics):\n",
    "#     return metrics[\"eval_overall_f1_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-06-17 21:37:22,337] A new study created in memory with name: no-name-31c206ac-059f-4419-8cd3-552656182f11\n",
      "Trial: {'learning_rate': 0.0002747421770686028, 'per_device_train_batch_size': 16, 'weight_decay': 0.0051135387555659}\n",
      "loading configuration file config.json from cache at /home/m13519061/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/config.json\n",
      "Model config MT5Config {\n",
      "  \"_name_or_path\": \"google/mt5-base\",\n",
      "  \"architectures\": [\n",
      "    \"MT5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"mt5\",\n",
      "  \"num_decoder_layers\": 12,\n",
      "  \"num_heads\": 12,\n",
      "  \"num_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"tokenizer_class\": \"T5Tokenizer\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250112\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/m13519061/.cache/huggingface/hub/models--google--mt5-base/snapshots/2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
      "/home/m13519061/anaconda3/envs/absa/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 17000\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 21260\n",
      "  Number of trainable parameters = 582401280\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1064' max='21260' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1064/21260 12:20 < 3:54:34, 1.43 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 39/125 00:04 < 00:09, 9.13 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0>          <extra_id_2>             <extra_id_2>     ;            <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-1063\n",
      "Configuration saved in ./output/run-0/checkpoint-1063/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-1063/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-1063/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-1063/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-1063/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-1063/spiece.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative <pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-2126\n",
      "Configuration saved in ./output/run-0/checkpoint-2126/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-2126/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-2126/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-2126/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-2126/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-2126/spiece.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-3189\n",
      "Configuration saved in ./output/run-0/checkpoint-3189/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-3189/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-3189/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-3189/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-3189/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-3189/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-1063] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-4252\n",
      "Configuration saved in ./output/run-0/checkpoint-4252/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-4252/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-4252/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-4252/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-4252/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-4252/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-2126] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-5315\n",
      "Configuration saved in ./output/run-0/checkpoint-5315/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-5315/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-5315/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-5315/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-5315/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-5315/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-3189] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-6378\n",
      "Configuration saved in ./output/run-0/checkpoint-6378/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-6378/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-6378/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-6378/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-6378/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-6378/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-4252] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-7441\n",
      "Configuration saved in ./output/run-0/checkpoint-7441/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-7441/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-7441/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-7441/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-7441/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-7441/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-5315] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-8504\n",
      "Configuration saved in ./output/run-0/checkpoint-8504/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-8504/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-8504/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-8504/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-8504/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-8504/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-6378] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-9567\n",
      "Configuration saved in ./output/run-0/checkpoint-9567/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-9567/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-9567/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-9567/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-9567/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-9567/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-7441] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-10630\n",
      "Configuration saved in ./output/run-0/checkpoint-10630/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-10630/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-10630/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-10630/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-10630/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-10630/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-8504] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan <extra_id_7>antal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-11693\n",
      "Configuration saved in ./output/run-0/checkpoint-11693/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-11693/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-11693/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-11693/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-11693/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-11693/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-9567] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-12756\n",
      "Configuration saved in ./output/run-0/checkpoint-12756/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-12756/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-12756/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-12756/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-12756/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-12756/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-10630] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-13819\n",
      "Configuration saved in ./output/run-0/checkpoint-13819/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-13819/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-13819/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-13819/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-13819/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-13819/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-11693] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-14882\n",
      "Configuration saved in ./output/run-0/checkpoint-14882/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-14882/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-14882/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-14882/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-14882/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-14882/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-12756] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-15945\n",
      "Configuration saved in ./output/run-0/checkpoint-15945/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-15945/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-15945/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-15945/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-15945/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-15945/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-13819] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-17008\n",
      "Configuration saved in ./output/run-0/checkpoint-17008/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-17008/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-17008/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-17008/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-17008/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-17008/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-15945] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-18071\n",
      "Configuration saved in ./output/run-0/checkpoint-18071/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-18071/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-18071/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-18071/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-18071/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-18071/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-14882] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-19134\n",
      "Configuration saved in ./output/run-0/checkpoint-19134/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-19134/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-19134/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-19134/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-19134/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-19134/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-17008] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUTS >> Ekstrak ABSA dengan format >> aspect : <extra_id_0>,opinion : <extra_id_1>,sentiment : <extra_id_2> | pelayanan memuaskan. cuma bantal airy kotor seperti tidak di cuci. dan tempat penyimpanan bantal biru airy kotor.</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "TARGETS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal biru airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "PREDS >> <extra_id_0> pelayanan <extra_id_1> memuaskan <extra_id_2> positive ; <extra_id_3> bantal airy <extra_id_4> kotor <extra_id_5> negative ; <extra_id_6> tempat penyimpanan bantal airu airy <extra_id_7> kotor <extra_id_8> negative</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/run-0/checkpoint-20197\n",
      "Configuration saved in ./output/run-0/checkpoint-20197/config.json\n",
      "Configuration saved in ./output/run-0/checkpoint-20197/config.json\n",
      "Model weights saved in ./output/run-0/checkpoint-20197/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/run-0/checkpoint-20197/tokenizer_config.json\n",
      "Special tokens file saved in ./output/run-0/checkpoint-20197/special_tokens_map.json\n",
      "Copy vocab file to ./output/run-0/checkpoint-20197/spiece.model\n",
      "Deleting older checkpoint [output/run-0/checkpoint-18071] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "# best_trial = trainer.hyperparameter_search(\n",
    "#     direction=\"maximize\",\n",
    "#     backend=\"optuna\",\n",
    "#     hp_space=optuna_hp_space,\n",
    "#     n_trials=20,\n",
    "#     # compute_objective=compute_objective\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_hparams = best_trial.hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"result_hparams.json\",'w') as fp:\n",
    "#     json.dump(result_hparams,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.train()\n",
    "\n",
    "# str_preds = generate_predictions(model, tokenizer_id, william_2[\"hotel\"][\"test\"][\"input\"], device, decoding_args)\n",
    "# preds = [catch_answer(el,\"aos\") for el in str_preds]\n",
    "str_preds = generate_predictions(model, tokenizer_id, william_tok[\"hotel\"][\"test\"], device, 16, 128, decoding_args)\n",
    "preds = [catch_answer(el,\"aos\") for el in str_preds]\n",
    "targets = [catch_answer(el,\"aos\") for el in william_2[\"hotel\"][\"test\"][\"output\"]]\n",
    "score = summary_score(preds,targets)\n",
    "print(f\"Score for OURS >>\", score)\n",
    "fname = \"OURS\"\n",
    "result = save_result(str_preds, preds, targets, fname + \"_pred.json\")\n",
    "with open(fname + \"_score.json\", 'w') as fp:\n",
    "    json.dump(score,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# !rm -rf ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "absa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
